"""
============================================
å¿«é€Ÿå¼€å§‹ï¼šä¸»é¢˜æŠ“å– + å­˜å‚¨
============================================
æ¼”ç¤ºå®Œæ•´çš„æŠ“å–å’Œå­˜å‚¨æµç¨‹
"""

import asyncio
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.x_crawl import TwitterCrawler, save_results


async def quick_start():
    """å¿«é€Ÿå¼€å§‹ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("ğŸš€ å¿«é€Ÿå¼€å§‹ï¼šä¸»é¢˜æŠ“å– + è‡ªåŠ¨å­˜å‚¨")
    print("=" * 60)
    
    crawler = TwitterCrawler()
    
    try:
        # æ­¥éª¤ 1ï¼šæœç´¢ä¸»é¢˜
        print("\nâ–¶ï¸ æ­¥éª¤ 1ï¼šæœç´¢æ¨æ–‡")
        query = "python programming"
        print(f"   æŸ¥è¯¢: {query}")
        
        try:
            results = await crawler.search_all_tweets(
                query=query,
                max_results=10
            )
            print(f"   âœ… æ‰¾åˆ° {results.result_count} æ¡æ¨æ–‡")
            
            # æ­¥éª¤ 2ï¼šè‡ªåŠ¨ä¿å­˜
            print("\nâ–¶ï¸ æ­¥éª¤ 2ï¼šä¿å­˜æ•°æ®")
            path = save_results(results, query, format="json")
            print(f"   âœ… æ–‡ä»¶: {path}")
            
            # æ­¥éª¤ 3ï¼šå±•ç¤ºå†…å®¹
            print("\nâ–¶ï¸ æ­¥éª¤ 3ï¼šé¢„è§ˆå†…å®¹")
            for i, tweet in enumerate(results.tweets[:3], 1):
                print(f"\n   {i}. {tweet.text[:80]}...")
                print(f"      ğŸ‘ {tweet.like_count or 0:,} | ğŸ”„ {tweet.retweet_count or 0:,}")
            
            if results.next_token:
                print(f"\n   ğŸ’¡ æç¤ºï¼šæœ‰æ›´å¤šç»“æœå¯ç”¨ (next_token å·²ä¿å­˜)")
            
        except Exception as e:
            if "429" in str(e):
                print(f"   âš ï¸ é€Ÿç‡é™åˆ¶ï¼šè¯·ç¨åå†è¯•")
            elif "Academic Research" in str(e):
                print(f"   âš ï¸ éœ€è¦ Academic Research æƒé™")
                print(f"   ğŸ’¡ å°è¯•ä½¿ç”¨å…¶ä»– APIï¼ˆå¦‚ get_tweetsï¼‰")
            else:
                print(f"   âŒ é”™è¯¯: {e}")
        
    finally:
        await crawler.close()
    
    print("\n" + "=" * 60)
    print("âœ… å®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ° data/ ç›®å½•")
    print("=" * 60)


async def alternative_demo():
    """å¤‡ç”¨æ¼”ç¤ºï¼ˆä½¿ç”¨ä¸éœ€è¦ç‰¹æ®Šæƒé™çš„ APIï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ¯ å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰¹é‡è·å–æ¨æ–‡")
    print("=" * 60)
    
    crawler = TwitterCrawler()
    
    try:
        # ä½¿ç”¨ get_tweetsï¼ˆä¸éœ€è¦ç‰¹æ®Šæƒé™ï¼‰
        print("\nâ–¶ï¸ è·å–ç‰¹å®šæ¨æ–‡")
        tweet_ids = ["20", "21", "22"]  # Twitter æ—©æœŸæ¨æ–‡
        
        try:
            results = await crawler.get_tweets(tweet_ids)
            print(f"   âœ… è·å–åˆ° {len(results.tweets)} æ¡æ¨æ–‡")
            
            # ä¿å­˜
            from src.x_crawl import save_tweets_json
            path = save_tweets_json(results.tweets, "early_tweets.json")
            print(f"   âœ… ä¿å­˜åˆ°: {path}")
            
            # å±•ç¤º
            for tweet in results.tweets:
                print(f"\n   ğŸ“„ {tweet.text}")
                print(f"      ç‚¹èµ: {tweet.like_count:,} | æ—¶é—´: {tweet.created_at}")
            
        except Exception as e:
            if "429" in str(e):
                print(f"   âš ï¸ é€Ÿç‡é™åˆ¶ï¼šè¯·ç­‰å¾… 15 åˆ†é’Ÿåé‡è¯•")
            else:
                print(f"   âŒ é”™è¯¯: {e}")
        
    finally:
        await crawler.close()
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    # å°è¯•ä¸»è¦æ–¹æ³•
    asyncio.run(quick_start())
    
    # å¦‚æœä¸»è¦æ–¹æ³•å¤±è´¥ï¼Œè¿è¡Œå¤‡ç”¨æ–¹æ¡ˆ
    print("\n" + "-" * 60)
    asyncio.run(alternative_demo())
