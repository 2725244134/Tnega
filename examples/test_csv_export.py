"""
============================================
æµ‹è¯• CSV å¯¼å‡ºåŠŸèƒ½ï¼ˆå¸¦è¯¦ç»†æ—¥å¿—ï¼‰
============================================
éªŒè¯æ¨¡å‹ä¿®æ”¹åçš„ CSV å¯¼å‡ºæ˜¯å¦æ­£å¸¸
é‡ç‚¹æ£€æŸ¥ author_name å­—æ®µæ˜¯å¦æ­£ç¡®æå–
"""

import asyncio
from pathlib import Path

from loguru import logger

from src.x_crawl import (
    collect_tweet_discussions,
    create_client,
    export_texts_from_collection,
)

# ============================================
# é…ç½®æ—¥å¿—
# ============================================

log_dir = Path(__file__).parent.parent / "logs"
log_dir.mkdir(exist_ok=True)

log_file = log_dir / "test_csv_export.log"

# ç§»é™¤é»˜è®¤è¾“å‡º
logger.remove()

# æ·»åŠ æ§åˆ¶å°è¾“å‡ºï¼ˆå½©è‰²ï¼‰
logger.add(
    lambda msg: print(msg, end=""),
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
    colorize=True,
    level="INFO",
)

# æ·»åŠ æ–‡ä»¶è¾“å‡ºï¼ˆå®Œæ•´ï¼‰
logger.add(
    log_file,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} | {message}",
    level="DEBUG",
    rotation="10 MB",
    encoding="utf-8",
)

logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")

# ============================================
# ä¸»æµ‹è¯•æµç¨‹
# ============================================


async def main():
    """
    å®Œæ•´æµ‹è¯•æµç¨‹ï¼š
    1. é‡‡é›†æ¨æ–‡æ•°æ®
    2. æ£€æŸ¥æ¨¡å‹å­—æ®µ
    3. å¯¼å‡º CSV
    4. éªŒè¯ CSV å†…å®¹
    """
    
    logger.info("=" * 80)
    logger.info("ğŸš€ å¼€å§‹æµ‹è¯• CSV å¯¼å‡ºåŠŸèƒ½")
    logger.info("=" * 80)
    
    # ============================================
    # æ­¥éª¤ 1: é‡‡é›†æ•°æ®
    # ============================================
    logger.info("\nğŸ“¡ æ­¥éª¤ 1: é‡‡é›†æ¨æ–‡æ•°æ®...")
    
    query = "(China parade OR 93é˜…å…µ) lang:ar"
    
    async with create_client() as client:
        result = await collect_tweet_discussions(
            query=query,
            client=client,
            query_type="Latest",
            max_seed_tweets=5,  # å°‘é‡æµ‹è¯•
            max_replies_per_tweet=3,
            include_thread=True,
            max_concurrent=2,
        )
    
    logger.info(f"\nâœ… é‡‡é›†å®Œæˆ:")
    logger.info(f"   ç§å­æ¨æ–‡: {result.metadata.seed_tweet_count}")
    logger.info(f"   æˆåŠŸå¤„ç†: {len(result.items)}")
    logger.info(f"   å¤±è´¥æ•°: {len(result.metadata.failed_tweet_ids)}")
    logger.info(f"   æ€»æ¨æ–‡æ•°: {result.total_tweets}")
    logger.info(f"   æ€»å›å¤æ•°: {result.total_replies}")
    logger.info(f"   Thread æ•°: {result.total_threads}")
    logger.info(f"   æˆåŠŸç‡: {result.success_rate:.1%}")
    
    # ============================================
    # æ­¥éª¤ 2: æ£€æŸ¥æ¨¡å‹å­—æ®µï¼ˆéªŒè¯ author_nameï¼‰
    # ============================================
    logger.info("\nğŸ” æ­¥éª¤ 2: æ£€æŸ¥æ¨¡å‹å­—æ®µ...")
    
    if not result.items:
        logger.error("âŒ æ²¡æœ‰é‡‡é›†åˆ°ä»»ä½•æ¨æ–‡ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return
    
    logger.info(f"\næ£€æŸ¥ç¬¬ 1 æ¡æ¨æ–‡çš„å­—æ®µ:")
    first_item = result.items[0]
    first_tweet = first_item.tweet
    
    logger.info(f"   Tweet.id: {first_tweet.id}")
    logger.info(f"   Tweet.text: {first_tweet.text[:50]}...")
    logger.info(f"   Tweet.author_name: {first_tweet.author_name}")  # âš ï¸ å…³é”®å­—æ®µ
    logger.info(f"   Tweet.created_at: {first_tweet.created_at}")
    logger.info(f"   Tweet.like_count: {first_tweet.like_count}")
    logger.info(f"   Tweet.retweet_count: {first_tweet.retweet_count}")
    logger.info(f"   Tweet.reply_count: {first_tweet.reply_count}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ author_name
    if first_tweet.author_name:
        logger.success(f"   âœ… author_name å­—æ®µå­˜åœ¨: '{first_tweet.author_name}'")
    else:
        logger.warning(f"   âš ï¸  author_name ä¸º None")
    
    # æ£€æŸ¥å›å¤å’Œ Thread
    if first_item.replies:
        logger.info(f"\næ£€æŸ¥ç¬¬ 1 æ¡å›å¤:")
        first_reply = first_item.replies[0]
        logger.info(f"   Reply.id: {first_reply.id}")
        logger.info(f"   Reply.text: {first_reply.text[:50]}...")
        logger.info(f"   Reply.author_name: {first_reply.author_name}")
        
        if first_reply.author_name:
            logger.success(f"   âœ… å›å¤çš„ author_name å­˜åœ¨: '{first_reply.author_name}'")
        else:
            logger.warning(f"   âš ï¸  å›å¤çš„ author_name ä¸º None")
    
    if first_item.thread_context:
        logger.info(f"\næ£€æŸ¥ç¬¬ 1 æ¡ Thread æ¨æ–‡:")
        first_thread = first_item.thread_context[0]
        logger.info(f"   Thread.id: {first_thread.id}")
        logger.info(f"   Thread.text: {first_thread.text[:50]}...")
        logger.info(f"   Thread.author_name: {first_thread.author_name}")
        
        if first_thread.author_name:
            logger.success(f"   âœ… Thread çš„ author_name å­˜åœ¨: '{first_thread.author_name}'")
        else:
            logger.warning(f"   âš ï¸  Thread çš„ author_name ä¸º None")
    
    # ç»Ÿè®¡ author_name çš„è¦†ç›–ç‡
    logger.info(f"\nğŸ“Š author_name å­—æ®µç»Ÿè®¡:")
    
    all_tweets = result.all_tweets
    tweets_with_author = sum(1 for t in all_tweets if t.author_name)
    tweets_without_author = len(all_tweets) - tweets_with_author
    
    logger.info(f"   æ€»æ¨æ–‡æ•°: {len(all_tweets)}")
    logger.info(f"   æœ‰ author_name: {tweets_with_author} ({tweets_with_author/len(all_tweets)*100:.1f}%)")
    logger.info(f"   æ—  author_name: {tweets_without_author} ({tweets_without_author/len(all_tweets)*100:.1f}%)")
    
    # ============================================
    # æ­¥éª¤ 3: å¯¼å‡º CSV
    # ============================================
    logger.info("\nğŸ’¾ æ­¥éª¤ 3: å¯¼å‡º CSV æ–‡ä»¶...")
    
    output_dir = Path("data/test_output")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    csv_path = output_dir / "test_export.csv"
    
    logger.info(f"   è¾“å‡ºè·¯å¾„: {csv_path}")
    
    try:
        export_texts_from_collection(
            collection=result,
            output_path=csv_path,
            file_format="csv",
            csv_mode="full",  # å®Œæ•´ç‰ˆæœ¬
            clean=True,
        )
        logger.success(f"   âœ… CSV å¯¼å‡ºæˆåŠŸ")
    except Exception as e:
        logger.error(f"   âŒ CSV å¯¼å‡ºå¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return
    
    # ============================================
    # æ­¥éª¤ 4: éªŒè¯ CSV æ–‡ä»¶
    # ============================================
    logger.info("\nâœ… æ­¥éª¤ 4: éªŒè¯ CSV æ–‡ä»¶...")
    
    if not csv_path.exists():
        logger.error(f"   âŒ CSV æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")
        return
    
    # è¯»å–æ–‡ä»¶å‰ 5 è¡Œ
    logger.info(f"   æ–‡ä»¶å¤§å°: {csv_path.stat().st_size} å­—èŠ‚")
    
    try:
        with open(csv_path, "r", encoding="utf-8-sig") as f:
            lines = [f.readline().strip() for _ in range(5)]
        
        logger.info(f"\n   CSV æ–‡ä»¶å‰ 5 è¡Œ:")
        for i, line in enumerate(lines, 1):
            # æˆªæ–­è¿‡é•¿çš„è¡Œ
            display_line = line if len(line) <= 100 else line[:100] + "..."
            logger.info(f"   {i}: {display_line}")
    except Exception as e:
        logger.error(f"   âŒ è¯»å– CSV æ–‡ä»¶å¤±è´¥: {e}")
        return
    
    # ä½¿ç”¨ csv æ¨¡å—éªŒè¯
    import csv
    
    try:
        with open(csv_path, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            rows = list(reader)
        
        logger.info(f"\n   CSV æ•°æ®éªŒè¯:")
        logger.info(f"   æ€»è¡Œæ•°: {len(rows)}")
        logger.info(f"   åˆ—å: {list(rows[0].keys()) if rows else 'æ— æ•°æ®'}")
        
        # æ£€æŸ¥å‰ 3 è¡Œçš„ä½œè€…åç§°
        logger.info(f"\n   å‰ 3 è¡Œçš„ä½œè€…åç§°:")
        for i, row in enumerate(rows[:3], 1):
            author = row.get("ä½œè€…åç§°", "N/A")
            source = row.get("æ¥æºç±»å‹", "N/A")
            content_preview = row.get("æ¨æ–‡å†…å®¹", "")[:30]
            logger.info(f"   {i}. [{source}] {author}: {content_preview}...")
        
        # ç»Ÿè®¡ä½œè€…åç§°åˆ†å¸ƒ
        author_names = [row.get("ä½œè€…åç§°", "Unknown") for row in rows]
        unknown_count = sum(1 for name in author_names if name == "Unknown")
        
        logger.info(f"\n   ä½œè€…åç§°ç»Ÿè®¡:")
        logger.info(f"   æ€»è¡Œæ•°: {len(rows)}")
        logger.info(f"   'Unknown': {unknown_count} ({unknown_count/len(rows)*100:.1f}%)")
        logger.info(f"   æœ‰åå­—: {len(rows) - unknown_count} ({(len(rows) - unknown_count)/len(rows)*100:.1f}%)")
        
        if unknown_count == 0:
            logger.success(f"   ğŸ‰ æ‰€æœ‰æ¨æ–‡éƒ½æœ‰ä½œè€…åç§°ï¼")
        elif unknown_count < len(rows) * 0.5:
            logger.success(f"   âœ… å¤§éƒ¨åˆ†æ¨æ–‡æœ‰ä½œè€…åç§°")
        else:
            logger.warning(f"   âš ï¸  è¶…è¿‡ä¸€åŠçš„æ¨æ–‡ä½œè€…ä¸º Unknown")
    
    except Exception as e:
        logger.error(f"   âŒ CSV éªŒè¯å¤±è´¥: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return
    
    # ============================================
    # æµ‹è¯•æ€»ç»“
    # ============================================
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    logger.info("=" * 80)
    logger.info(f"âœ… æ•°æ®é‡‡é›†: æˆåŠŸ")
    logger.info(f"âœ… æ¨¡å‹å­—æ®µ: author_name å­˜åœ¨")
    logger.info(f"âœ… CSV å¯¼å‡º: æˆåŠŸ")
    logger.info(f"âœ… CSV éªŒè¯: é€šè¿‡")
    logger.info(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶: {csv_path}")
    logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    asyncio.run(main())
