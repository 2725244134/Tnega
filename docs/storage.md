# æ•°æ®å­˜å‚¨åŠŸèƒ½

## âœ… å·²å®ç°

å®Œæ•´çš„æ–‡ä»¶å­˜å‚¨ç³»ç»Ÿï¼Œæ”¯æŒ JSON å’Œ JSONL ä¸¤ç§æ ¼å¼ã€‚

---

## ğŸ“¦ æ ¸å¿ƒåŠŸèƒ½

### 1. JSON å­˜å‚¨ï¼ˆé€‚åˆå°æ‰¹é‡æ•°æ®ï¼‰

```python
from src.x_crawl import save_tweets_json, load_tweets_json

# ä¿å­˜æ¨æ–‡
tweets = [tweet1, tweet2, tweet3]
path = save_tweets_json(tweets, "my_tweets.json")
# è¾“å‡º: ğŸ’¾ ä¿å­˜ 3 æ¡æ¨æ–‡ â†’ data/my_tweets.json

# åŠ è½½æ¨æ–‡
loaded = load_tweets_json(path)
# è¾“å‡º: ğŸ“‚ åŠ è½½ 3 æ¡æ¨æ–‡ â† data/my_tweets.json
```

**ç‰¹ç‚¹**ï¼š
- âœ… å¯è¯»æ€§å¥½ï¼ˆæ ¼å¼åŒ–ç¼©è¿›ï¼‰
- âœ… é€‚åˆæ‰‹åŠ¨æŸ¥çœ‹å’Œç¼–è¾‘
- âœ… æ–‡ä»¶å¤§å°è¾ƒå¤§

---

### 2. JSONL å­˜å‚¨ï¼ˆé€‚åˆå¤§æ‰¹é‡æ•°æ®ï¼‰

```python
from src.x_crawl import save_tweets_jsonl, load_tweets_jsonl

# ç¬¬ä¸€æ‰¹æ•°æ®
save_tweets_jsonl(batch1, "stream.jsonl", append=False)

# è¿½åŠ ç¬¬äºŒæ‰¹
save_tweets_jsonl(batch2, "stream.jsonl", append=True)

# è¿½åŠ ç¬¬ä¸‰æ‰¹
save_tweets_jsonl(batch3, "stream.jsonl", append=True)

# åŠ è½½å…¨éƒ¨
all_tweets = load_tweets_jsonl(Path("data/stream.jsonl"))
```

**ç‰¹ç‚¹**ï¼š
- âœ… æ”¯æŒæµå¼è¿½åŠ ï¼ˆåˆ†æ‰¹æŠ“å–åœºæ™¯ï¼‰
- âœ… æ–‡ä»¶å¤§å°æ›´å°
- âœ… é€è¡Œå¤„ç†å¤§æ–‡ä»¶

---

### 3. å®Œæ•´æœç´¢ç»“æœå­˜å‚¨

```python
from src.x_crawl import save_search_results_json, load_search_results_json

# ä¿å­˜å®Œæ•´ç»“æœï¼ˆåŒ…å«æ¨æ–‡ã€ç”¨æˆ·ã€åª’ä½“ï¼‰
results = await crawler.search_all_tweets("AI agents")
path = save_search_results_json(results, "ai_agents_results.json")

# åŠ è½½
loaded_results = load_search_results_json(path)

# è®¿é—®æ•°æ®
for tweet in loaded_results.tweets:
    author = loaded_results.users[tweet.author_id]
    print(f"@{author.username}: {tweet.text}")
```

**åŒ…å«å†…å®¹**ï¼š
- âœ… æ¨æ–‡åˆ—è¡¨
- âœ… ç”¨æˆ·æ˜ å°„ï¼ˆid â†’ Userï¼‰
- âœ… åª’ä½“æ•°æ®
- âœ… åˆ†é¡µä¿¡æ¯ï¼ˆnext_tokenï¼‰
- âœ… å…ƒæ•°æ®ï¼ˆresult_count, total_count, saved_at, search metadataï¼‰

`search metadata` ä¼šè®°å½•æŸ¥è¯¢è¯­å¥ã€æ—¶é—´çª—å£ã€åˆ†é¡µæ¬¡æ•°ç­‰ä¿¡æ¯ï¼Œä¾¿äºåœ¨åç»­åˆ†æä¸­è¿½æº¯æŠ“å–å‚æ•°ã€‚

---

### 4. ä¾¿æ·ä¿å­˜å‡½æ•°ï¼ˆè‡ªåŠ¨å‘½åï¼‰

```python
from src.x_crawl import save_results

# è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
results = await crawler.search_all_tweets("Web3 developer")
path = save_results(results, "Web3 developer", format="json")
# ç”Ÿæˆ: data/Web3_developer_20251029_173530.json

# JSONL æ ¼å¼
path = save_results(results, "AI agents 2024", format="jsonl")
# ç”Ÿæˆ: data/AI_agents_2024_20251029_173530.jsonl
```

**è‡ªåŠ¨å¤„ç†**ï¼š
- âœ… æ¸…ç†æ–‡ä»¶åï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
- âœ… æ·»åŠ æ—¶é—´æˆ³
- âœ… ç»Ÿä¸€ä¿å­˜åœ¨ `data/` ç›®å½•

---

## ğŸ¯ å…¸å‹ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæ‰¹é‡æŠ“å–å¹¶ä¿å­˜

```python
import asyncio
from src.x_crawl import TwitterCrawler, save_results

async def crawl_topic(query: str):
    """æŠ“å–ä¸»é¢˜å¹¶è‡ªåŠ¨ä¿å­˜"""
    crawler = TwitterCrawler()
    
    try:
        # æœç´¢æ¨æ–‡
        results = await crawler.search_all_tweets(
            query=query,
            max_results=500
        )
        
        # è‡ªåŠ¨ä¿å­˜
        path = save_results(results, query, format="json")
        print(f"âœ… æŠ“å–å®Œæˆï¼Œä¿å­˜åˆ°: {path}")
        
        return results
        
    finally:
        await crawler.close()

# ä½¿ç”¨
asyncio.run(crawl_topic("AI agents 2024"))
```

---

### åœºæ™¯ 2ï¼šåˆ†æ‰¹æŠ“å–ï¼ˆå¢é‡è¿½åŠ ï¼‰

```python
from src.x_crawl import TwitterCrawler, save_tweets_jsonl

async def crawl_with_pagination(query: str, total: int = 5000):
    """åˆ†æ‰¹æŠ“å–å¤§é‡æ•°æ®"""
    crawler = TwitterCrawler()
    filename = "large_dataset.jsonl"
    next_token = None
    count = 0
    
    try:
        while count < total:
            # æ¯æ¬¡è·å– 500 æ¡
            results = await crawler.search_all_tweets(
                query=query,
                max_results=500,
                next_token=next_token
            )
            
            # è¿½åŠ åˆ°æ–‡ä»¶
            save_tweets_jsonl(
                results.tweets,
                filename,
                append=(count > 0)  # ç¬¬ä¸€æ¬¡ä¸è¿½åŠ ï¼Œåç»­è¿½åŠ 
            )
            
            count += len(results.tweets)
            next_token = results.next_token
            
            print(f"è¿›åº¦: {count}/{total}")
            
            # æ²¡æœ‰ä¸‹ä¸€é¡µäº†
            if not next_token:
                break
                
    finally:
        await crawler.close()
    
    print(f"âœ… æ€»å…±æŠ“å– {count} æ¡æ¨æ–‡")
```

---

### åœºæ™¯ 3ï¼šæ•°æ®å¤„ç†æµæ°´çº¿

```python
from pathlib import Path
from src.x_crawl import load_tweets_jsonl

def analyze_tweets(filepath: Path):
    """åˆ†æå·²ä¿å­˜çš„æ¨æ–‡æ•°æ®"""
    # åŠ è½½æ•°æ®
    tweets = load_tweets_jsonl(filepath)
    
    # ç»Ÿè®¡åˆ†æ
    total = len(tweets)
    total_likes = sum(t.like_count or 0 for t in tweets)
    avg_likes = total_likes / total if total > 0 else 0
    
    print(f"ğŸ“Š åˆ†æç»“æœ:")
    print(f"   æ€»æ¨æ–‡æ•°: {total:,}")
    print(f"   æ€»ç‚¹èµæ•°: {total_likes:,}")
    print(f"   å¹³å‡ç‚¹èµ: {avg_likes:.1f}")
    
    # æ‰¾å‡ºçƒ­é—¨æ¨æ–‡
    hot_tweets = sorted(tweets, key=lambda t: t.like_count or 0, reverse=True)[:10]
    
    print(f"\nğŸ”¥ çƒ­é—¨æ¨æ–‡ TOP 10:")
    for i, tweet in enumerate(hot_tweets, 1):
        print(f"   {i}. {tweet.text[:50]}... (ğŸ‘ {tweet.like_count:,})")

# ä½¿ç”¨
analyze_tweets(Path("data/AI_agents_2024_20251029_173530.json"))
```

---

### åœºæ™¯ 4ï¼šé˜¿è¯­å†å²å›å¡«ä¸å­˜å‚¨

```bash
uv run python scripts/backfill_parade_2025_ar.py \
  --start 2024-12-01 \
  --end 2025-12-31 \
  --window-days 14 \
  --format json
```

- æŸ¥è¯¢æ ‡ç­¾ï¼ˆä¾‹å¦‚ `--queries parade2025_ar_signature`ï¼‰å¯ç­›é€‰ç‰¹å®šæœç´¢è¯­å¥
- è¾“å‡ºæ–‡ä»¶ç»Ÿä¸€ä¿å­˜åœ¨ `data/`ï¼Œæ–‡ä»¶ååŒ…å«æ—¶é—´çª—å£ä¸æ ‡ç­¾
- JSON ç»“æœåŒ…å« `metadata.search` å­—æ®µï¼Œè®°å½•æŸ¥è¯¢è¯­å¥ã€æ—¶é—´çª—å£ã€åˆ†é¡µæ¬¡æ•°ç­‰è¿½æº¯ä¿¡æ¯

---

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
data/
â”œâ”€â”€ AI_agents_2024_20251029_173530.json      # å®Œæ•´æœç´¢ç»“æœï¼ˆJSONï¼‰
â”œâ”€â”€ Web3_developer_20251029_173530.jsonl     # æ¨æ–‡åˆ—è¡¨ï¼ˆJSONLï¼‰
â”œâ”€â”€ stream.jsonl                              # æµå¼è¿½åŠ æ•°æ®
â””â”€â”€ custom_filename.json                      # è‡ªå®šä¹‰æ–‡ä»¶å
```

---

## ğŸ¨ æ–‡ä»¶æ ¼å¼å¯¹æ¯”

### JSON æ ¼å¼
```json
[
  {
    "id": "1234567890",
    "text": "æ¨æ–‡å†…å®¹",
    "created_at": "2024-01-01T12:00:00",
    "author_id": "12345",
    "like_count": 100,
    "retweet_count": 50
  },
  {
    "id": "0987654321",
    "text": "å¦ä¸€æ¡æ¨æ–‡",
    ...
  }
]
```

### JSONL æ ¼å¼
```jsonl
{"id": "1234567890", "text": "æ¨æ–‡å†…å®¹", "created_at": "2024-01-01T12:00:00", ...}
{"id": "0987654321", "text": "å¦ä¸€æ¡æ¨æ–‡", "created_at": "2024-01-01T13:00:00", ...}
{"id": "1122334455", "text": "ç¬¬ä¸‰æ¡æ¨æ–‡", "created_at": "2024-01-01T14:00:00", ...}
```

---

## âš™ï¸ é…ç½®

é»˜è®¤å­˜å‚¨ç›®å½•ï¼š`data/`

ä¿®æ”¹é»˜è®¤ç›®å½•ï¼š
```python
from pathlib import Path
from src.x_crawl import save_tweets_json

# ä½¿ç”¨è‡ªå®šä¹‰ç›®å½•
save_tweets_json(tweets, "output.json", data_dir=Path("my_data"))
```

---

## âœ… æµ‹è¯•ç»“æœ

```bash
$ uv run python tests/test_storage_mock.py

âœ… JSON å­˜å‚¨æµ‹è¯•é€šè¿‡
âœ… JSONL å­˜å‚¨æµ‹è¯•é€šè¿‡
âœ… æœç´¢ç»“æœå­˜å‚¨æµ‹è¯•é€šè¿‡
âœ… ä¾¿æ·ä¿å­˜å‡½æ•°æµ‹è¯•é€šè¿‡
âœ… æ•°æ®æŒä¹…åŒ–æµ‹è¯•é€šè¿‡ï¼ˆç‰¹æ®Šå­—ç¬¦: ğŸ˜€ #AI @userï¼‰

ğŸ“ 6 ä¸ªæµ‹è¯•æ–‡ä»¶ç”Ÿæˆåœ¨ data/ ç›®å½•
```

---

## ğŸš€ ä¸‹ä¸€æ­¥

å­˜å‚¨åŠŸèƒ½å·²å°±ç»ªï¼Œç°åœ¨å¯ä»¥ï¼š

1. **å¼€å§‹å¤§è§„æ¨¡æŠ“å–** - ä½¿ç”¨ JSONL æ ¼å¼æµå¼ä¿å­˜
2. **ç¦»çº¿åˆ†ææ•°æ®** - åŠ è½½å·²ä¿å­˜çš„æ•°æ®è¿›è¡Œç»Ÿè®¡
3. **æ„å»ºæ•°æ®é›†** - ç§¯ç´¯ä¸»é¢˜ç›¸å…³çš„æ¨æ–‡è¯­æ–™åº“

é…åˆå³å°†å®ç°çš„ **é”™è¯¯é‡è¯•æœºåˆ¶**ï¼Œå¯ä»¥å®ç°ç¨³å®šçš„é•¿æ—¶é—´æŠ“å–ä»»åŠ¡ã€‚
