"""
============================================
Twitter æ•°æ®é‡‡é›†æ ¸å¿ƒæ¨¡å—
============================================
åŸºäºŽ tweepy çš„å¼‚æ­¥ API å°è£…
æ‰€æœ‰æ–¹æ³•è¿”å›žç±»åž‹å®‰å…¨çš„ Pydantic æ¨¡åž‹
"""

from __future__ import annotations

import os
from typing import Any

from dotenv import load_dotenv
from loguru import logger
from tweepy.asynchronous import AsyncClient #pyright:ignore[reportMissingTypeStubs]

from .models import User, Tweet, SearchResults


# ============================================
# é…ç½®åŠ è½½
# ============================================

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
load_dotenv()


def _load_bearer_token() -> str:
    """
    ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½ Bearer Tokenï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    
    å‰æï¼šå·²é€šè¿‡ load_dotenv() åŠ è½½ .env æ–‡ä»¶
    """
    # ç»Ÿä¸€è½¬å°å†™åŒ¹é…ï¼Œå¹¶åŽ»é™¤é”®åä¸­çš„å¼•å·
    env_dict_lower = {k.lower().strip('"').strip("'"): v for k, v in os.environ.items()}
    
    if "bearer_token" in env_dict_lower:
        logger.info("âœ… Bearer Token åŠ è½½æˆåŠŸ")
        return env_dict_lower["bearer_token"]
    
    raise ValueError(
        "æœªæ‰¾åˆ° BEARER_TOKEN çŽ¯å¢ƒå˜é‡\n"
        "è¯·è®¾ç½®çŽ¯å¢ƒå˜é‡æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶"
    )


# ============================================
# Twitter API å®¢æˆ·ç«¯
# ============================================

class TwitterCrawler:
    """
    Twitter API å¼‚æ­¥å®¢æˆ·ç«¯
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å°è£… tweepy AsyncClient
    2. å°† API å“åº”è½¬æ¢ä¸º Pydantic æ¨¡åž‹
    3. å¤„ç†é”™è¯¯å’Œé€ŸçŽ‡é™åˆ¶
    """
    
    def __init__(self, bearer_token: str | None = None):
        """
        åˆå§‹åŒ– Twitter å®¢æˆ·ç«¯
        
        Args:
            bearer_token: Twitter API Bearer Token (ä¸æä¾›åˆ™è‡ªåŠ¨ä»Ž .env åŠ è½½)
        """
        if bearer_token is None:
            bearer_token = _load_bearer_token()
        
        self._client = AsyncClient(bearer_token=bearer_token)
        logger.info("ðŸš€ TwitterCrawler åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿žæŽ¥"""
        # tweepy AsyncClient å½“å‰ç‰ˆæœ¬ä¸éœ€è¦æ˜¾å¼å…³é—­
        logger.info("ðŸ”Œ TwitterCrawler è¿žæŽ¥å…³é—­")
    
    # ============================================
    # æ ¸å¿ƒ API - ä¸»é¢˜æŠ“å–ä¸“ç”¨
    # ============================================
    
    async def search_all_tweets(
        self,
        query: str,
        max_results: int = 100,
        start_time: str | None = None,
        end_time: str | None = None,
        next_token: str | None = None
    ) -> SearchResults:
        """
        æœç´¢å®Œæ•´åŽ†å²æŽ¨æ–‡ï¼ˆéœ€è¦ Academic Research æƒé™ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢ï¼ˆTwitter æœç´¢è¯­æ³•ï¼Œæœ€å¤š 1024 å­—ç¬¦ï¼‰
            max_results: è¿”å›žç»“æžœæ•° (10-500ï¼Œé»˜è®¤ 100)
            start_time: èµ·å§‹æ—¶é—´ (ISO 8601 æ ¼å¼: YYYY-MM-DDTHH:mm:ssZ)
            end_time: ç»“æŸæ—¶é—´ (ISO 8601 æ ¼å¼)
            next_token: åˆ†é¡µä»¤ç‰Œï¼ˆç”¨äºŽèŽ·å–ä¸‹ä¸€é¡µï¼‰
        
        Returns:
            SearchResults å¯¹è±¡
        
        Example:
            >>> # æœç´¢ 2023 å¹´å…³äºŽ AI çš„æŽ¨æ–‡
            >>> results = await crawler.search_all_tweets(
            ...     "AI agents",
            ...     max_results=500,
            ...     start_time="2023-01-01T00:00:00Z",
            ...     end_time="2023-12-31T23:59:59Z"
            ... )
        
        Note:
            - é»˜è®¤è¿”å›žæœ€è¿‘ 30 å¤©çš„æŽ¨æ–‡ï¼ˆå¦‚ä¸æŒ‡å®š start_timeï¼‰
            - éœ€è¦ Academic Research Track æƒé™
            - æŽ¨æ–‡ä»Ž 2006-03-26 é¦–æ¡æŽ¨æ–‡å¼€å§‹å¯æœç´¢
        """
        logger.info(f"ðŸ“¡ æœç´¢å®Œæ•´åŽ†å²: query='{query}', max_results={max_results}")
        
        response = await self._client.search_all_tweets(
            query=query,
            max_results=min(max_results, 500),
            start_time=start_time,
            end_time=end_time,
            next_token=next_token,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "referenced_tweets", "attachments"
            ],
            expansions=["author_id", "attachments.media_keys", "referenced_tweets.id"],
            user_fields=["id", "username", "name", "verified", "public_metrics"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æŽ¨æ–‡: {query}")
            return SearchResults(
                tweets=[],
                users={},
                media={},
                result_count=0
            )
        
        # è§£æžæŽ¨æ–‡
        tweets = [self._parse_tweet_data(tweet) for tweet in response.data]
        
        # è§£æžç”¨æˆ·
        users = {}
        if response.includes and "users" in response.includes:
            for user in response.includes["users"]:
                user_data = self._parse_user_data(user)
                users[user_data["id"]] = User(**user_data)
        
        # è§£æžåª’ä½“
        media = {}
        if response.includes and "media" in response.includes:
            for m in response.includes["media"]:
                media_data = self._parse_media_data(m)
                media[media_data["media_key"]] = media_data
        
        meta = response.meta or {}
        
        results = SearchResults(
            tweets=[Tweet(**t) for t in tweets],
            users=users,
            media=media,
            next_token=meta.get("next_token"),
            result_count=meta.get("result_count", len(tweets))
        )
        
        logger.success(f"âœ… æœç´¢æˆåŠŸ: {results.result_count} æ¡æŽ¨æ–‡")
        return results
    
    async def get_tweet(self, tweet_id: str) -> Tweet:
        """
        èŽ·å–å•æ¡æŽ¨æ–‡è¯¦æƒ…ï¼ˆåŒ…å«å®Œæ•´çš„å¼•ç”¨/è¯„è®ºå…³ç³»ï¼‰
        
        Args:
            tweet_id: æŽ¨æ–‡ ID
        
        Returns:
            Tweet å¯¹è±¡
        
        Example:
            >>> tweet = await crawler.get_tweet("1234567890")
            >>> print(f"å†…å®¹: {tweet.text}")
            >>> print(f"ç‚¹èµž: {tweet.like_count}, è½¬å‘: {tweet.retweet_count}")
        """
        logger.info(f"ðŸ“¡ èŽ·å–æŽ¨æ–‡è¯¦æƒ…: tweet_id={tweet_id}")
        
        response = await self._client.get_tweet(
            id=tweet_id,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "source", "referenced_tweets", "attachments"
            ],
            expansions=["author_id", "attachments.media_keys", "referenced_tweets.id"],
            user_fields=["id", "username", "name", "verified"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            raise ValueError(f"æŽ¨æ–‡ä¸å­˜åœ¨ (ID: {tweet_id})")
        
        tweet_data = self._parse_tweet_data(response.data)
        tweet = Tweet(**tweet_data)
        
        logger.success(f"âœ… èŽ·å–æŽ¨æ–‡æˆåŠŸ: {tweet.id}")
        return tweet
    
    async def get_tweets(self, tweet_ids: list[str]) -> SearchResults:
        """
        æ‰¹é‡èŽ·å–æŽ¨æ–‡è¯¦æƒ…ï¼ˆæœ€å¤š 100 æ¡ï¼‰
        
        Args:
            tweet_ids: æŽ¨æ–‡ ID åˆ—è¡¨ï¼ˆæœ€å¤š 100 ä¸ªï¼‰
        
        Returns:
            SearchResults å¯¹è±¡ï¼ˆåŒ…å«æŽ¨æ–‡ã€ç”¨æˆ·ã€åª’ä½“æ˜ å°„ï¼‰
        
        Example:
            >>> # æ‰¹é‡èŽ·å–çƒ­é—¨æŽ¨æ–‡çš„è¯¦æƒ…
            >>> results = await crawler.get_tweets([
            ...     "1234567890",
            ...     "0987654321",
            ...     "1122334455"
            ... ])
            >>> for tweet in results.tweets:
            ...     print(f"{tweet.text} - ç‚¹èµž: {tweet.like_count}")
        
        Note:
            é€‚ç”¨äºŽèŽ·å–æœç´¢ç»“æžœä¸­æåˆ°çš„å¼•ç”¨æŽ¨æ–‡æˆ–è¯„è®º
        """
        logger.info(f"ðŸ“¡ æ‰¹é‡èŽ·å–æŽ¨æ–‡: count={len(tweet_ids)}")
        
        if len(tweet_ids) > 100:
            logger.warning("âš ï¸ æŽ¨æ–‡ ID æ•°é‡è¶…è¿‡ 100ï¼Œæˆªå–å‰ 100 ä¸ª")
            tweet_ids = tweet_ids[:100]
        
        response = await self._client.get_tweets(
            ids=tweet_ids,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "source", "referenced_tweets", "attachments"
            ],
            expansions=["author_id", "attachments.media_keys"],
            user_fields=["id", "username", "name", "verified", "public_metrics"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æŽ¨æ–‡")
            return SearchResults(
                tweets=[],
                users={},
                media={},
                result_count=0
            )
        
        # è§£æžæŽ¨æ–‡
        tweets = [self._parse_tweet_data(tweet) for tweet in response.data]
        
        # è§£æžç”¨æˆ·
        users = {}
        if response.includes and "users" in response.includes:
            for user in response.includes["users"]:
                user_data = self._parse_user_data(user)
                users[user_data["id"]] = User(**user_data)
        
        # è§£æžåª’ä½“
        media = {}
        if response.includes and "media" in response.includes:
            for m in response.includes["media"]:
                media_data = self._parse_media_data(m)
                media[media_data["media_key"]] = media_data
        
        results = SearchResults(
            tweets=[Tweet(**t) for t in tweets],
            users=users,
            media=media,
            result_count=len(tweets)
        )
        
        logger.success(f"âœ… æ‰¹é‡èŽ·å–æˆåŠŸ: {results.result_count} æ¡æŽ¨æ–‡")
        return results
    
    async def fetch_user_by_id(self, user_id: str) -> User:
        """
        æ ¹æ®ç”¨æˆ· ID èŽ·å–ç”¨æˆ·ä¿¡æ¯
        
        Args:
            user_id: Twitter ç”¨æˆ· ID
        
        Returns:
            User å¯¹è±¡
        
        Example:
            >>> user = await crawler.fetch_user_by_id("12")
            >>> print(f"@{user.username}: {user.followers_count:,} ç²‰ä¸")
        """
        logger.info(f"ðŸ“¡ èŽ·å–ç”¨æˆ·ä¿¡æ¯ (ID: {user_id})")
        
        response = await self._client.get_user(
            id=user_id,
            user_fields=[
                "id", "username", "name", "created_at",
                "description", "location", "verified",
                "profile_image_url", "public_metrics"
            ]
        )
        
        if not response.data:
            raise ValueError(f"ç”¨æˆ·ä¸å­˜åœ¨ (ID: {user_id})")
        
        user_data = self._parse_user_data(response.data)
        user = User(**user_data)
        
        logger.success(f"âœ… èŽ·å–ç”¨æˆ·æˆåŠŸ: @{user.username}")
        return user
    
    # ============================================
    # æ•°æ®è§£æžå™¨ï¼ˆAPI å“åº” -> dictï¼‰
    # ============================================
    
    def _parse_user_data(self, user: Any) -> dict[str, Any]:
        """å°† tweepy User å¯¹è±¡è½¬æ¢ä¸º dict"""
        public_metrics = getattr(user, "public_metrics", None) or {}
        
        return {
            "id": str(user.id),
            "username": user.username,
            "name": user.name,
            "created_at": getattr(user, "created_at", None),
            "description": getattr(user, "description", None),
            "location": getattr(user, "location", None),
            "verified": getattr(user, "verified", None),
            "profile_image_url": getattr(user, "profile_image_url", None),
            "followers_count": public_metrics.get("followers_count"),
            "following_count": public_metrics.get("following_count"),
            "tweet_count": public_metrics.get("tweet_count"),
            "listed_count": public_metrics.get("listed_count"),
        }
    
    def _parse_tweet_data(self, tweet: Any) -> dict[str, Any]:
        """å°† tweepy Tweet å¯¹è±¡è½¬æ¢ä¸º dict"""
        public_metrics = getattr(tweet, "public_metrics", None) or {}
        
        return {
            "id": str(tweet.id),
            "text": tweet.text,
            "created_at": tweet.created_at,
            "author_id": str(tweet.author_id),
            "retweet_count": public_metrics.get("retweet_count"),
            "reply_count": public_metrics.get("reply_count"),
            "like_count": public_metrics.get("like_count"),
            "quote_count": public_metrics.get("quote_count"),
            "impression_count": public_metrics.get("impression_count"),
            "lang": getattr(tweet, "lang", None),
            "possibly_sensitive": getattr(tweet, "possibly_sensitive", None),
            "source": getattr(tweet, "source", None),
            "referenced_tweets": getattr(tweet, "referenced_tweets", None),
            "attachments": getattr(tweet, "attachments", None),
            "in_reply_to_user_id": None,  # éœ€è¦ä»Ž referenced_tweets è§£æž
        }
    
    def _parse_media_data(self, media: Any) -> dict[str, Any]:
        """å°† tweepy Media å¯¹è±¡è½¬æ¢ä¸º dict"""
        return {
            "media_key": media.media_key,
            "type": media.type,
            "url": getattr(media, "url", None),
            "preview_image_url": getattr(media, "preview_image_url", None),
            "width": getattr(media, "width", None),
            "height": getattr(media, "height", None),
            "duration_ms": getattr(media, "duration_ms", None),
        }


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

async def create_crawler() -> TwitterCrawler:
    """åˆ›å»ºå¹¶è¿”å›ž TwitterCrawler å®žä¾‹"""
    return TwitterCrawler()
