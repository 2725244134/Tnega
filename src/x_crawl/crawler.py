"""
============================================
Twitter æ•°æ®é‡‡é›†æ ¸å¿ƒæ¨¡å—
============================================
åŸºäºŽ tweepy çš„å¼‚æ­¥ API å°è£…
æ‰€æœ‰æ–¹æ³•è¿”å›žç±»åž‹å®‰å…¨çš„ Pydantic æ¨¡åž‹
"""

from __future__ import annotations

import asyncio
import os
from datetime import datetime, timezone
from typing import Any

from dotenv import load_dotenv
from loguru import logger
from tweepy.asynchronous import AsyncClient #pyright:ignore[reportMissingTypeStubs]
from tweepy.errors import TooManyRequests

from .models import User, Tweet, SearchResults, SearchMetadata


# ============================================
# é…ç½®åŠ è½½
# ============================================

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
load_dotenv()


def _load_bearer_token() -> str:
    """
    ä»ŽçŽ¯å¢ƒå˜é‡åŠ è½½ Bearer Tokenï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    
    å‰æï¼šå·²é€šè¿‡ load_dotenv() åŠ è½½ .env æ–‡ä»¶
    """
    # ç»Ÿä¸€è½¬å°å†™åŒ¹é…ï¼Œå¹¶åŽ»é™¤é”®åä¸­çš„å¼•å·
    env_dict_lower = {k.lower().strip('"').strip("'"): v for k, v in os.environ.items()}
    
    if "bearer_token" in env_dict_lower:
        logger.info("âœ… Bearer Token åŠ è½½æˆåŠŸ")
        return env_dict_lower["bearer_token"]
    
    raise ValueError(
        "æœªæ‰¾åˆ° BEARER_TOKEN çŽ¯å¢ƒå˜é‡\n"
        "è¯·è®¾ç½®çŽ¯å¢ƒå˜é‡æˆ–åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶"
    )


# ============================================
# å·¥å…·å‡½æ•°
# ============================================

def _parse_iso_datetime(value: str | None) -> datetime | None:
    """Parse ISO 8601 strings (including trailing Z) into aware datetimes."""
    if value is None:
        return None

    try:
        if value.endswith("Z"):
            return datetime.fromisoformat(value.replace("Z", "+00:00"))
        return datetime.fromisoformat(value)
    except ValueError:
        logger.warning(f"âš ï¸ æ— æ³•è§£æžæ—¶é—´å­—ç¬¦ä¸²: {value}")
        return None


def _coerce_iso_string(value: datetime | str | None) -> str | None:
    """Ensure datetime inputs are rendered as ISO 8601 strings with Z suffix."""
    if value is None:
        return None

    if isinstance(value, datetime):
        if value.tzinfo is None:
            value = value.replace(tzinfo=timezone.utc)
        return value.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

    return value


def _extract_lang_token(query: str | None) -> str | None:
    """Extract lang:xx token from a query string if present."""
    if not query:
        return None

    parts = query.split()
    for part in parts:
        token = part.strip().lower()
        if token.startswith("lang:") and len(token) > 5:
            return token.split(":", 1)[1]
    return None


def _normalize_datetime(value: datetime | None) -> datetime:
    """Coerce datetimes to timezone-aware UTC for sorting and metadata."""
    if value is None:
        return datetime.min.replace(tzinfo=timezone.utc)
    if value.tzinfo is None:
        return value.replace(tzinfo=timezone.utc)
    return value.astimezone(timezone.utc)


# ============================================
# Twitter API å®¢æˆ·ç«¯
# ============================================

class TwitterCrawler:
    """
    Twitter API å¼‚æ­¥å®¢æˆ·ç«¯
    
    æ ¸å¿ƒèŒè´£ï¼š
    1. å°è£… tweepy AsyncClient
    2. å°† API å“åº”è½¬æ¢ä¸º Pydantic æ¨¡åž‹
    3. å¤„ç†é”™è¯¯å’Œé€ŸçŽ‡é™åˆ¶
    """
    
    def __init__(self, bearer_token: str | None = None):
        """
        åˆå§‹åŒ– Twitter å®¢æˆ·ç«¯
        
        Args:
            bearer_token: Twitter API Bearer Token (ä¸æä¾›åˆ™è‡ªåŠ¨ä»Ž .env åŠ è½½)
        """
        if bearer_token is None:
            bearer_token = _load_bearer_token()
        
        self._client = AsyncClient(bearer_token=bearer_token)
        logger.info("ðŸš€ TwitterCrawler åˆå§‹åŒ–å®Œæˆ")
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿žæŽ¥"""
        # tweepy AsyncClient å½“å‰ç‰ˆæœ¬ä¸éœ€è¦æ˜¾å¼å…³é—­
        logger.info("ðŸ”Œ TwitterCrawler è¿žæŽ¥å…³é—­")
    
    # ============================================
    # æ ¸å¿ƒ API - ä¸»é¢˜æŠ“å–ä¸“ç”¨
    # ============================================
    
    async def search_all_tweets(
        self,
        query: str,
        max_results: int = 100,
        start_time: str | None = None,
        end_time: str | None = None,
        next_token: str | None = None
    ) -> SearchResults:
        """
        æœç´¢å®Œæ•´åŽ†å²æŽ¨æ–‡ï¼ˆéœ€è¦ Academic Research æƒé™ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢ï¼ˆTwitter æœç´¢è¯­æ³•ï¼Œæœ€å¤š 1024 å­—ç¬¦ï¼‰
            max_results: è¿”å›žç»“æžœæ•° (10-500ï¼Œé»˜è®¤ 100)
            start_time: èµ·å§‹æ—¶é—´ (ISO 8601 æ ¼å¼: YYYY-MM-DDTHH:mm:ssZ)
            end_time: ç»“æŸæ—¶é—´ (ISO 8601 æ ¼å¼)
            next_token: åˆ†é¡µä»¤ç‰Œï¼ˆç”¨äºŽèŽ·å–ä¸‹ä¸€é¡µï¼‰
        
        Returns:
            SearchResults å¯¹è±¡
        
        Example:
            >>> # æœç´¢ 2023 å¹´å…³äºŽ AI çš„æŽ¨æ–‡
            >>> results = await crawler.search_all_tweets(
            ...     "AI agents",
            ...     max_results=500,
            ...     start_time="2023-01-01T00:00:00Z",
            ...     end_time="2023-12-31T23:59:59Z"
            ... )
        
        Note:
            - é»˜è®¤è¿”å›žæœ€è¿‘ 30 å¤©çš„æŽ¨æ–‡ï¼ˆå¦‚ä¸æŒ‡å®š start_timeï¼‰
            - éœ€è¦ Academic Research Track æƒé™
            - æŽ¨æ–‡ä»Ž 2006-03-26 é¦–æ¡æŽ¨æ–‡å¼€å§‹å¯æœç´¢
        """
        logger.info(f"ðŸ“¡ æœç´¢å®Œæ•´åŽ†å²: query='{query}', max_results={max_results}")
        
        response = await self._client.search_all_tweets(
            query=query,
            max_results=min(max_results, 500),
            start_time=start_time,
            end_time=end_time,
            next_token=next_token,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "referenced_tweets", "attachments", "conversation_id",
                "context_annotations", "entities", "geo", "in_reply_to_user_id"
            ],
            expansions=["author_id", "attachments.media_keys", "referenced_tweets.id"],
            user_fields=["id", "username", "name", "verified", "public_metrics"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æŽ¨æ–‡: {query}")
            return SearchResults(
                tweets=[],
                users={},
                media={},
                result_count=0
            )
        
        # è§£æžæŽ¨æ–‡
        tweets = [self._parse_tweet_data(tweet) for tweet in response.data]
        
        # è§£æžç”¨æˆ·
        users = {}
        if response.includes and "users" in response.includes:
            for user in response.includes["users"]:
                user_data = self._parse_user_data(user)
                users[user_data["id"]] = User(**user_data)
        
        # è§£æžåª’ä½“
        media = {}
        if response.includes and "media" in response.includes:
            for m in response.includes["media"]:
                media_data = self._parse_media_data(m)
                media[media_data["media_key"]] = media_data
        
        meta = response.meta or {}
        
        total_count = meta.get("total_tweet_count")
        metadata = SearchMetadata(
            query=query,
            start_time=_parse_iso_datetime(start_time),
            end_time=_parse_iso_datetime(end_time),
            source="search_all",
            language=_extract_lang_token(query),
            page_count=1,
            total_collected=meta.get("result_count", len(tweets)),
            request_parameters={
                "max_results": max_results,
                "next_token": next_token,
                "start_time": start_time,
                "end_time": end_time,
            },
        )

        results = SearchResults(
            tweets=[Tweet(**t) for t in tweets],
            users=users,
            media=media,
            next_token=meta.get("next_token"),
            result_count=meta.get("result_count", len(tweets)),
            total_count=total_count,
            metadata=metadata,
        )

        logger.success(f"âœ… æœç´¢æˆåŠŸ: {results.result_count} æ¡æŽ¨æ–‡")
        return results

    async def search_recent_tweets(
        self,
        query: str,
        max_results: int = 10,
        start_time: str | None = None,
        end_time: str | None = None,
        next_token: str | None = None
    ) -> SearchResults:
        """
        æœç´¢æœ€è¿‘ 7 å¤©çš„æŽ¨æ–‡ï¼ˆä¸éœ€è¦ç‰¹æ®Šæƒé™ï¼‰
        
        Args:
            query: æœç´¢æŸ¥è¯¢ï¼ˆTwitter æœç´¢è¯­æ³•ï¼‰
            max_results: è¿”å›žç»“æžœæ•° (10-100ï¼Œé»˜è®¤ 10)
            start_time: èµ·å§‹æ—¶é—´ (ISO 8601 æ ¼å¼ï¼Œæœ€å¤š 7 å¤©å‰)
            end_time: ç»“æŸæ—¶é—´ (ISO 8601 æ ¼å¼)
            next_token: åˆ†é¡µä»¤ç‰Œ
        
        Returns:
            SearchResults å¯¹è±¡
        
        Example:
            >>> # æœç´¢æœ€è¿‘çš„æŽ¨æ–‡
            >>> results = await crawler.search_recent_tweets(
            ...     "China military parade",
            ...     max_results=100
            ... )
        
        Note:
            - åªèƒ½æœç´¢æœ€è¿‘ 7 å¤©çš„æŽ¨æ–‡
            - ä¸éœ€è¦ Academic Research æƒé™
            - é€‚åˆæµ‹è¯•å’Œå°è§„æ¨¡æ•°æ®é‡‡é›†
        """
        logger.info(f"ðŸ“¡ æœç´¢æœ€è¿‘æŽ¨æ–‡: query='{query}', max_results={max_results}")
        
        response = await self._client.search_recent_tweets(
            query=query,
            max_results=min(max_results, 100),
            start_time=start_time,
            end_time=end_time,
            next_token=next_token,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "referenced_tweets", "attachments", "conversation_id",
                "context_annotations", "entities", "geo", "in_reply_to_user_id"
            ],
            expansions=["author_id", "attachments.media_keys", "referenced_tweets.id"],
            user_fields=["id", "username", "name", "verified", "public_metrics"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…æŽ¨æ–‡: {query}")
            return SearchResults(
                tweets=[],
                users={},
                media={},
                result_count=0
            )
        
        # è§£æžæŽ¨æ–‡
        tweets = [self._parse_tweet_data(tweet) for tweet in response.data]
        
        # è§£æžç”¨æˆ·
        users = {}
        if response.includes and "users" in response.includes:
            for user in response.includes["users"]:
                user_data = self._parse_user_data(user)
                users[user_data["id"]] = User(**user_data)
        
        # è§£æžåª’ä½“
        media = {}
        if response.includes and "media" in response.includes:
            for m in response.includes["media"]:
                media_data = self._parse_media_data(m)
                media[media_data["media_key"]] = media_data
        
        meta = response.meta or {}
        
        total_count = meta.get("total_tweet_count")
        metadata = SearchMetadata(
            query=query,
            start_time=_parse_iso_datetime(start_time),
            end_time=_parse_iso_datetime(end_time),
            source="search_recent",
            language=_extract_lang_token(query),
            page_count=1,
            total_collected=meta.get("result_count", len(tweets)),
            request_parameters={
                "max_results": max_results,
                "next_token": next_token,
                "start_time": start_time,
                "end_time": end_time,
            },
        )

        results = SearchResults(
            tweets=[Tweet(**t) for t in tweets],
            users=users,
            media=media,
            next_token=meta.get("next_token"),
            result_count=meta.get("result_count", len(tweets)),
            total_count=total_count,
            metadata=metadata,
        )

        logger.success(f"âœ… æœç´¢æˆåŠŸ: {results.result_count} æ¡æŽ¨æ–‡")
        return results

    async def search_all_tweets_paginated(
        self,
        query: str,
        *,
        start_time: datetime | str | None = None,
        end_time: datetime | str | None = None,
        max_results: int = 500,
        max_pages: int | None = None,
        page_pause: float = 2.0,
        label: str | None = None,
        language: str | None = None,
    ) -> SearchResults:
        """
        è‡ªåŠ¨åˆ†é¡µæŠ“å–åŽ†å²æŽ¨æ–‡å¹¶åˆå¹¶ç»“æžœã€‚

        Args:
            query: Twitter æœç´¢è¯­å¥
            start_time: èµ·å§‹æ—¶é—´ï¼ˆdatetime æˆ– ISO å­—ç¬¦ä¸²ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆdatetime æˆ– ISO å­—ç¬¦ä¸²ï¼‰
            max_results: å•é¡µæŠ“å–æ•°é‡ï¼ˆ10-500ï¼‰
            max_pages: é™åˆ¶åˆ†é¡µæ¬¡æ•°ï¼ˆNone è¡¨ç¤ºå°½å¯èƒ½å¤šï¼‰
            page_pause: æ¯é¡µæŠ“å–åŽçš„ç­‰å¾…ç§’æ•°ï¼Œç¼“è§£é€ŸçŽ‡é™åˆ¶
            label: æ•°æ®é›†æ ‡ç­¾ï¼Œç”¨äºŽè®°å½• metadata
            language: å¦‚æžœæä¾›ä¸”æŸ¥è¯¢ä¸­æœªåŒ…å« lang: è¿‡æ»¤ï¼Œåˆ™è‡ªåŠ¨è¿½åŠ 

        Returns:
            åˆå¹¶åŽçš„æœç´¢ç»“æžœ
        """

        prepared_query = query.strip()
        language = language or _extract_lang_token(prepared_query)
        if language and "lang:" not in prepared_query.lower():
            prepared_query = f"{prepared_query} lang:{language}"

        start_iso = _coerce_iso_string(start_time)
        end_iso = _coerce_iso_string(end_time)

        all_tweets: dict[str, Tweet] = {}
        aggregated_users: dict[str, User] = {}
        aggregated_media: dict[str, Any] = {}

        next_token: str | None = None
        page_count = 0
        approximate_total: int | None = None
        final_next_token: str | None = None

        retry = 0

        while True:
            if max_pages is not None and page_count >= max_pages:
                logger.info("ðŸ›‘ è¾¾åˆ°åˆ†é¡µä¸Šé™ï¼Œåœæ­¢æŠ“å–")
                break

            try:
                page = await self.search_all_tweets(
                    query=prepared_query,
                    max_results=max_results,
                    start_time=start_iso,
                    end_time=end_iso,
                    next_token=next_token,
                )
            except TooManyRequests:
                wait_seconds = min(900, 60 * (2 ** retry))
                retry += 1
                logger.warning(f"â³ é€ŸçŽ‡é™åˆ¶ï¼Œç­‰å¾… {wait_seconds}s åŽé‡è¯• (page={page_count + 1})")
                await asyncio.sleep(wait_seconds)
                continue

            retry = 0
            page_count += 1

            approximate_total = page.total_count or approximate_total
            final_next_token = page.next_token

            for tweet in page.tweets:
                all_tweets[tweet.id] = tweet

            aggregated_users.update(page.users)
            aggregated_media.update(page.media)

            if not page.next_token:
                break

            next_token = page.next_token

            if page_pause > 0:
                await asyncio.sleep(page_pause)

        if not all_tweets:
            logger.warning(f"âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•æŽ¨æ–‡: {prepared_query}")

        sorted_tweets = sorted(
            all_tweets.values(),
            key=lambda t: _normalize_datetime(t.created_at),
        )

        metadata = SearchMetadata(
            query=prepared_query,
            label=label,
            start_time=_parse_iso_datetime(start_iso),
            end_time=_parse_iso_datetime(end_iso),
            source="search_all",
            language=language,
            page_count=page_count,
            total_collected=len(sorted_tweets),
            request_parameters={
                "max_results": max_results,
                "max_pages": max_pages,
                "page_pause": page_pause,
                "start_time": start_iso,
                "end_time": end_iso,
            },
        )

        aggregated_results = SearchResults(
            tweets=sorted_tweets,
            users=aggregated_users,
            media=aggregated_media,
            next_token=final_next_token,
            result_count=len(sorted_tweets),
            total_count=approximate_total,
            metadata=metadata,
        )

        logger.success(
            "ðŸ“š åŽ†å²æŠ“å–å®Œæˆ: %s | æŽ¨æ–‡ %s æ¡ | åˆ†é¡µ %s",
            label or prepared_query,
            len(sorted_tweets),
            page_count,
        )

        return aggregated_results
    
    async def get_tweet(self, tweet_id: str) -> Tweet:
        """
        èŽ·å–å•æ¡æŽ¨æ–‡è¯¦æƒ…ï¼ˆåŒ…å«å®Œæ•´çš„å¼•ç”¨/è¯„è®ºå…³ç³»ï¼‰
        
        Args:
            tweet_id: æŽ¨æ–‡ ID
        
        Returns:
            Tweet å¯¹è±¡
        
        Example:
            >>> tweet = await crawler.get_tweet("1234567890")
            >>> print(f"å†…å®¹: {tweet.text}")
            >>> print(f"ç‚¹èµž: {tweet.like_count}, è½¬å‘: {tweet.retweet_count}")
        """
        logger.info(f"ðŸ“¡ èŽ·å–æŽ¨æ–‡è¯¦æƒ…: tweet_id={tweet_id}")
        
        response = await self._client.get_tweet(
            id=tweet_id,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "source", "referenced_tweets", "attachments"
            ],
            expansions=["author_id", "attachments.media_keys", "referenced_tweets.id"],
            user_fields=["id", "username", "name", "verified"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            raise ValueError(f"æŽ¨æ–‡ä¸å­˜åœ¨ (ID: {tweet_id})")
        
        tweet_data = self._parse_tweet_data(response.data)
        tweet = Tweet(**tweet_data)
        
        logger.success(f"âœ… èŽ·å–æŽ¨æ–‡æˆåŠŸ: {tweet.id}")
        return tweet
    
    async def get_tweets(self, tweet_ids: list[str]) -> SearchResults:
        """
        æ‰¹é‡èŽ·å–æŽ¨æ–‡è¯¦æƒ…ï¼ˆæœ€å¤š 100 æ¡ï¼‰
        
        Args:
            tweet_ids: æŽ¨æ–‡ ID åˆ—è¡¨ï¼ˆæœ€å¤š 100 ä¸ªï¼‰
        
        Returns:
            SearchResults å¯¹è±¡ï¼ˆåŒ…å«æŽ¨æ–‡ã€ç”¨æˆ·ã€åª’ä½“æ˜ å°„ï¼‰
        
        Example:
            >>> # æ‰¹é‡èŽ·å–çƒ­é—¨æŽ¨æ–‡çš„è¯¦æƒ…
            >>> results = await crawler.get_tweets([
            ...     "1234567890",
            ...     "0987654321",
            ...     "1122334455"
            ... ])
            >>> for tweet in results.tweets:
            ...     print(f"{tweet.text} - ç‚¹èµž: {tweet.like_count}")
        
        Note:
            é€‚ç”¨äºŽèŽ·å–æœç´¢ç»“æžœä¸­æåˆ°çš„å¼•ç”¨æŽ¨æ–‡æˆ–è¯„è®º
        """
        logger.info(f"ðŸ“¡ æ‰¹é‡èŽ·å–æŽ¨æ–‡: count={len(tweet_ids)}")
        
        if len(tweet_ids) > 100:
            logger.warning("âš ï¸ æŽ¨æ–‡ ID æ•°é‡è¶…è¿‡ 100ï¼Œæˆªå–å‰ 100 ä¸ª")
            tweet_ids = tweet_ids[:100]
        
        response = await self._client.get_tweets(
            ids=tweet_ids,
            tweet_fields=[
                "id", "text", "created_at", "author_id",
                "public_metrics", "lang", "possibly_sensitive",
                "source", "referenced_tweets", "attachments"
            ],
            expansions=["author_id", "attachments.media_keys"],
            user_fields=["id", "username", "name", "verified", "public_metrics"],
            media_fields=["media_key", "type", "url", "preview_image_url"]
        )
        
        if not response.data:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æŽ¨æ–‡")
            return SearchResults(
                tweets=[],
                users={},
                media={},
                result_count=0
            )
        
        # è§£æžæŽ¨æ–‡
        tweets = [self._parse_tweet_data(tweet) for tweet in response.data]
        
        # è§£æžç”¨æˆ·
        users = {}
        if response.includes and "users" in response.includes:
            for user in response.includes["users"]:
                user_data = self._parse_user_data(user)
                users[user_data["id"]] = User(**user_data)
        
        # è§£æžåª’ä½“
        media = {}
        if response.includes and "media" in response.includes:
            for m in response.includes["media"]:
                media_data = self._parse_media_data(m)
                media[media_data["media_key"]] = media_data
        
        results = SearchResults(
            tweets=[Tweet(**t) for t in tweets],
            users=users,
            media=media,
            result_count=len(tweets)
        )
        
        logger.success(f"âœ… æ‰¹é‡èŽ·å–æˆåŠŸ: {results.result_count} æ¡æŽ¨æ–‡")
        return results
    
    async def fetch_user_by_id(self, user_id: str) -> User:
        """
        æ ¹æ®ç”¨æˆ· ID èŽ·å–ç”¨æˆ·ä¿¡æ¯
        
        Args:
            user_id: Twitter ç”¨æˆ· ID
        
        Returns:
            User å¯¹è±¡
        
        Example:
            >>> user = await crawler.fetch_user_by_id("12")
            >>> print(f"@{user.username}: {user.followers_count:,} ç²‰ä¸")
        """
        logger.info(f"ðŸ“¡ èŽ·å–ç”¨æˆ·ä¿¡æ¯ (ID: {user_id})")
        
        response = await self._client.get_user(
            id=user_id,
            user_fields=[
                "id", "username", "name", "created_at",
                "description", "location", "verified",
                "profile_image_url", "public_metrics"
            ]
        )
        
        if not response.data:
            raise ValueError(f"ç”¨æˆ·ä¸å­˜åœ¨ (ID: {user_id})")
        
        user_data = self._parse_user_data(response.data)
        user = User(**user_data)
        
        logger.success(f"âœ… èŽ·å–ç”¨æˆ·æˆåŠŸ: @{user.username}")
        return user
    
    # ============================================
    # æ•°æ®è§£æžå™¨ï¼ˆAPI å“åº” -> dictï¼‰
    # ============================================
    
    def _parse_user_data(self, user: Any) -> dict[str, Any]:
        """å°† tweepy User å¯¹è±¡è½¬æ¢ä¸º dict"""
        public_metrics = getattr(user, "public_metrics", None) or {}
        
        return {
            "id": str(user.id),
            "username": user.username,
            "name": user.name,
            "created_at": getattr(user, "created_at", None),
            "description": getattr(user, "description", None),
            "location": getattr(user, "location", None),
            "verified": getattr(user, "verified", None),
            "profile_image_url": getattr(user, "profile_image_url", None),
            "followers_count": public_metrics.get("followers_count"),
            "following_count": public_metrics.get("following_count"),
            "tweet_count": public_metrics.get("tweet_count"),
            "listed_count": public_metrics.get("listed_count"),
        }
    
    def _parse_tweet_data(self, tweet: Any) -> dict[str, Any]:
        """å°† tweepy Tweet å¯¹è±¡è½¬æ¢ä¸º dict"""
        public_metrics = getattr(tweet, "public_metrics", None) or {}
        
        return {
            "id": str(tweet.id),
            "text": tweet.text,
            "created_at": tweet.created_at,
            "author_id": str(tweet.author_id),
            "retweet_count": public_metrics.get("retweet_count"),
            "reply_count": public_metrics.get("reply_count"),
            "like_count": public_metrics.get("like_count"),
            "quote_count": public_metrics.get("quote_count"),
            "impression_count": public_metrics.get("impression_count"),
            "lang": getattr(tweet, "lang", None),
            "possibly_sensitive": getattr(tweet, "possibly_sensitive", None),
            "source": getattr(tweet, "source", None),
            "referenced_tweets": getattr(tweet, "referenced_tweets", None),
            "attachments": getattr(tweet, "attachments", None),
            "in_reply_to_user_id": getattr(tweet, "in_reply_to_user_id", None),
            "conversation_id": getattr(tweet, "conversation_id", None),
            "context_annotations": getattr(tweet, "context_annotations", None),
            "entities": getattr(tweet, "entities", None),
            "geo": getattr(tweet, "geo", None),
        }
    
    def _parse_media_data(self, media: Any) -> dict[str, Any]:
        """å°† tweepy Media å¯¹è±¡è½¬æ¢ä¸º dict"""
        return {
            "media_key": media.media_key,
            "type": media.type,
            "url": getattr(media, "url", None),
            "preview_image_url": getattr(media, "preview_image_url", None),
            "width": getattr(media, "width", None),
            "height": getattr(media, "height", None),
            "duration_ms": getattr(media, "duration_ms", None),
        }


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

async def create_crawler() -> TwitterCrawler:
    """åˆ›å»ºå¹¶è¿”å›ž TwitterCrawler å®žä¾‹"""
    return TwitterCrawler()
