"""
============================================
æ•°æ®å­˜å‚¨æ¨¡å—
============================================
å°†çˆ¬å–çš„æ¨æ–‡æ•°æ®ä¿å­˜åˆ°æ–‡ä»¶
æ”¯æŒ JSON å’Œ JSONL æ ¼å¼
"""

from __future__ import annotations

import json
from datetime import datetime, timezone
from pathlib import Path

from loguru import logger

from .models import Tweet, User, SearchResults, SearchMetadata


# ============================================
# å­˜å‚¨è·¯å¾„é…ç½®
# ============================================

DEFAULT_DATA_DIR = Path("data")


def _ensure_dir(path: Path) -> Path:
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    path.mkdir(parents=True, exist_ok=True)
    return path


# ============================================
# JSON å­˜å‚¨ï¼ˆé€‚åˆå°æ‰¹é‡æ•°æ®ï¼‰
# ============================================

def save_tweets_json(
    tweets: list[Tweet],
    filename: str | None = None,
    data_dir: Path = DEFAULT_DATA_DIR
) -> Path:
    """
    ä¿å­˜æ¨æ–‡åˆ—è¡¨ä¸º JSON æ–‡ä»¶
    
    Args:
        tweets: æ¨æ–‡åˆ—è¡¨
        filename: æ–‡ä»¶åï¼ˆä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ï¼‰
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    
    Example:
        >>> tweets = [tweet1, tweet2, tweet3]
        >>> path = save_tweets_json(tweets)
        >>> print(f"ä¿å­˜åˆ°: {path}")
    """
    _ensure_dir(data_dir)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"tweets_{timestamp}.json"
    
    filepath = data_dir / filename
    
    # è½¬æ¢ä¸º dict åˆ—è¡¨
    data = [tweet.model_dump(mode="json") for tweet in tweets]
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.success(f"ğŸ’¾ ä¿å­˜ {len(tweets)} æ¡æ¨æ–‡ â†’ {filepath}")
    return filepath


def save_search_results_json(
    results: SearchResults,
    filename: str | None = None,
    data_dir: Path = DEFAULT_DATA_DIR
) -> Path:
    """
    ä¿å­˜æœç´¢ç»“æœä¸º JSON æ–‡ä»¶ï¼ˆåŒ…å«æ¨æ–‡ã€ç”¨æˆ·ã€åª’ä½“ï¼‰
    
    Args:
        results: SearchResults å¯¹è±¡
        filename: æ–‡ä»¶å
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    _ensure_dir(data_dir)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"search_results_{timestamp}.json"
    
    filepath = data_dir / filename
    
    # è½¬æ¢ä¸ºå®Œæ•´çš„æ•°æ®ç»“æ„
    metadata_payload: dict[str, Any] = {
        "result_count": results.result_count,
        "total_count": results.total_count,
        "next_token": results.next_token,
        "saved_at": datetime.now(timezone.utc).isoformat(),
    }

    if results.metadata:
        metadata_payload["search"] = results.metadata.model_dump(mode="json")

    data = {
        "tweets": [t.model_dump(mode="json") for t in results.tweets],
        "users": {uid: u.model_dump(mode="json") for uid, u in results.users.items()},
        "media": results.media,  # media å·²ç»æ˜¯ dictï¼Œå¯ä»¥ç›´æ¥åºåˆ—åŒ–
        "metadata": metadata_payload,
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.success(f"ğŸ’¾ ä¿å­˜æœç´¢ç»“æœ â†’ {filepath} ({results.result_count} æ¡æ¨æ–‡)")
    return filepath


# ============================================
# JSONL å­˜å‚¨ï¼ˆé€‚åˆå¤§æ‰¹é‡æ•°æ®ï¼‰
# ============================================

def save_tweets_jsonl(
    tweets: list[Tweet],
    filename: str | None = None,
    data_dir: Path = DEFAULT_DATA_DIR,
    append: bool = False
) -> Path:
    """
    ä¿å­˜æ¨æ–‡ä¸º JSONL æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€æ¡æ¨æ–‡ï¼‰
    
    Args:
        tweets: æ¨æ–‡åˆ—è¡¨
        filename: æ–‡ä»¶å
        data_dir: æ•°æ®ç›®å½•
        append: æ˜¯å¦è¿½åŠ åˆ°å·²æœ‰æ–‡ä»¶ï¼ˆé€‚åˆåˆ†æ‰¹æŠ“å–ï¼‰
    
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    
    Example:
        >>> # ç¬¬ä¸€æ‰¹
        >>> save_tweets_jsonl(batch1, "ai_tweets.jsonl")
        >>> # ç¬¬äºŒæ‰¹ï¼ˆè¿½åŠ ï¼‰
        >>> save_tweets_jsonl(batch2, "ai_tweets.jsonl", append=True)
    
    Note:
        JSONL æ ¼å¼é€‚åˆæµå¼å¤„ç†å’Œå¢é‡å†™å…¥
    """
    _ensure_dir(data_dir)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"tweets_{timestamp}.jsonl"
    
    filepath = data_dir / filename
    mode = "a" if append else "w"
    
    with open(filepath, mode, encoding="utf-8") as f:
        for tweet in tweets:
            line = json.dumps(tweet.model_dump(mode="json"), ensure_ascii=False)
            f.write(line + "\n")
    
    action = "è¿½åŠ " if append else "ä¿å­˜"
    logger.success(f"ğŸ’¾ {action} {len(tweets)} æ¡æ¨æ–‡ â†’ {filepath}")
    return filepath


def append_tweet_jsonl(
    tweet: Tweet,
    filename: str,
    data_dir: Path = DEFAULT_DATA_DIR
) -> Path:
    """
    è¿½åŠ å•æ¡æ¨æ–‡åˆ° JSONL æ–‡ä»¶ï¼ˆæµå¼å†™å…¥ï¼‰
    
    Args:
        tweet: å•æ¡æ¨æ–‡
        filename: æ–‡ä»¶å
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        æ–‡ä»¶è·¯å¾„
    
    Example:
        >>> async for tweet in stream_tweets():
        ...     append_tweet_jsonl(tweet, "stream.jsonl")
    """
    return save_tweets_jsonl([tweet], filename, data_dir, append=True)


# ============================================
# æ•°æ®åŠ è½½
# ============================================

def load_tweets_json(filepath: Path) -> list[Tweet]:
    """
    ä» JSON æ–‡ä»¶åŠ è½½æ¨æ–‡åˆ—è¡¨
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    with open(filepath, encoding="utf-8") as f:
        data = json.load(f)
    
    tweets = [Tweet(**item) for item in data]
    logger.info(f"ğŸ“‚ åŠ è½½ {len(tweets)} æ¡æ¨æ–‡ â† {filepath}")
    return tweets


def load_tweets_jsonl(filepath: Path) -> list[Tweet]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½æ¨æ–‡åˆ—è¡¨
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ¨æ–‡åˆ—è¡¨
    """
    tweets = []
    with open(filepath, encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data = json.loads(line)
                tweets.append(Tweet(**data))
    
    logger.info(f"ğŸ“‚ åŠ è½½ {len(tweets)} æ¡æ¨æ–‡ â† {filepath}")
    return tweets


def load_search_results_json(filepath: Path) -> SearchResults:
    """
    ä» JSON æ–‡ä»¶åŠ è½½æœç´¢ç»“æœ
    
    Args:
        filepath: æ–‡ä»¶è·¯å¾„
    
    Returns:
        SearchResults å¯¹è±¡
    """
    with open(filepath, encoding="utf-8") as f:
        data = json.load(f)
    
    metadata_blob = data.get("metadata", {})
    search_metadata_blob = metadata_blob.get("search")
    search_metadata = SearchMetadata(**search_metadata_blob) if search_metadata_blob else None

    results = SearchResults(
        tweets=[Tweet(**t) for t in data["tweets"]],
        users={uid: User(**u) for uid, u in data["users"].items()},
        media=data.get("media", {}),
        result_count=metadata_blob.get("result_count", len(data.get("tweets", []))),
        total_count=metadata_blob.get("total_count"),
        next_token=metadata_blob.get("next_token"),
        metadata=search_metadata,
    )
    
    logger.info(f"ğŸ“‚ åŠ è½½æœç´¢ç»“æœ â† {filepath} ({results.result_count} æ¡æ¨æ–‡)")
    return results


# ============================================
# ä¾¿æ·å‡½æ•°
# ============================================

def save_results(
    results: SearchResults,
    query: str,
    format: str = "json",
    data_dir: Path = DEFAULT_DATA_DIR
) -> Path:
    """
    è‡ªåŠ¨ä¿å­˜æœç´¢ç»“æœï¼ˆæ ¹æ®æŸ¥è¯¢ç”Ÿæˆæ–‡ä»¶åï¼‰
    
    Args:
        results: æœç´¢ç»“æœ
        query: æœç´¢æŸ¥è¯¢ï¼ˆç”¨äºç”Ÿæˆæ–‡ä»¶åï¼‰
        format: æ–‡ä»¶æ ¼å¼ï¼ˆ"json" æˆ– "jsonl"ï¼‰
        data_dir: æ•°æ®ç›®å½•
    
    Returns:
        ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    
    Example:
        >>> results = await crawler.search_all_tweets("AI agents")
        >>> save_results(results, "AI agents")
    """
    # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ä½œä¸ºæ–‡ä»¶å
    safe_query = "".join(c if c.isalnum() or c in (" ", "_") else "_" for c in query)
    safe_query = safe_query.strip().replace(" ", "_")[:50]  # é™åˆ¶é•¿åº¦

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if results.metadata:
        results.metadata.label = results.metadata.label or safe_query
        results.metadata.total_collected = results.result_count

    if format == "jsonl":
        filename = f"{safe_query}_{timestamp}.jsonl"
        return save_tweets_jsonl(results.tweets, filename, data_dir)
    else:
        filename = f"{safe_query}_{timestamp}.json"
        return save_search_results_json(results, filename, data_dir)
