"""
============================================
93 é˜…å…µï¼ˆ2025ï¼‰é˜¿è¯­æ¨æ–‡å†å²å›å¡«è„šæœ¬
============================================
é’ˆå¯¹é˜¿æ‹‰ä¼¯è¯­ç”¨æˆ·çš„è®¨è®ºï¼Œæ‰¹é‡æŠ“å–å¹¶ä¿å­˜åˆ° data/ ç›®å½•
"""

from __future__ import annotations

import argparse
import asyncio
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from typing import Iterable

from loguru import logger

from src.x_crawl import TwitterCrawler, save_results


@dataclass(frozen=True)
class QuerySpec:
    """æè¿°ä¸€æ¬¡æœç´¢ä»»åŠ¡"""

    label: str
    query: str


DEFAULT_QUERIES: tuple[QuerySpec, ...] = (
    QuerySpec(
        label="parade2025_ar_signature",
        query='(Ø§Ù„ØµÙŠÙ† OR Ø¨ÙƒÙŠÙ†) ("Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ" OR "Ø§Ø³ØªØ¹Ø±Ø§Ø¶ Ø¹Ø³ÙƒØ±ÙŠ") (2025 OR "Ø°ÙƒØ±Ù‰ Ø§Ù„Ù†ØµØ±")',
    ),
    QuerySpec(
        label="parade2025_ar_victory",
        query='("ÙŠÙˆÙ… Ø§Ù„Ù†ØµØ±" OR "Ø°ÙƒØ±Ù‰ Ø§Ù„Ù†ØµØ±") (Ø§Ù„ØµÙŠÙ† OR Ø¨ÙƒÙŠÙ†) (Ø§Ù„Ø¹Ø±Ø¶)',
    ),
    QuerySpec(
        label="parade2025_ar_hashtags",
        query='(#Ø§Ù„ØµÙŠÙ† OR #Ø§Ù„Ø¹Ø±Ø¶_Ø§Ù„Ø¹Ø³ÙƒØ±ÙŠ OR #Ø¨ÙƒÙŠÙ†) (2025 OR "Ø°ÙƒØ±Ù‰")',
    ),
    QuerySpec(
        label="parade2025_ar_coalition",
        query='(Ø§Ù„ØµÙŠÙ† OR Ø¨ÙƒÙŠÙ†) (Ø§Ù„Ø¹Ø±Ø¶ OR Ø§Ù„Ø§Ø­ØªÙØ§Ù„) (Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ OR Ø§Ù„Ø®Ù„ÙŠØ¬ OR Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© OR Ø§Ù„Ø§Ù…Ø§Ø±Ø§Øª)',
    ),
)


def parse_date(value: str) -> datetime:
    """Parse YYYY-MM-DD string to aware datetime at start of day UTC."""
    dt = datetime.fromisoformat(value)
    if dt.tzinfo is None:
        dt = dt.replace(tzinfo=timezone.utc)
    return dt


def iter_windows(start: datetime, end: datetime, window_days: int) -> Iterable[tuple[datetime, datetime]]:
    """Yield sliding windows within [start, end)."""
    cursor = start
    delta = timedelta(days=window_days)

    while cursor < end:
        window_end = min(cursor + delta, end)
        yield cursor, window_end
        cursor = window_end


async def backfill_window(
    crawler: TwitterCrawler,
    spec: QuerySpec,
    window_start: datetime,
    window_end: datetime,
    *,
    max_pages: int | None,
    max_results: int,
    page_pause: float,
    output_format: str,
) -> None:
    iso_start = window_start.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")
    iso_end = window_end.astimezone(timezone.utc).isoformat().replace("+00:00", "Z")

    window_label = f"{spec.label}_{window_start:%Y%m%d}_{window_end:%Y%m%d}"
    logger.info(
        "ğŸ›°ï¸ å¼€å§‹æŠ“å–: %s | %s â†’ %s", spec.label, iso_start, iso_end
    )

    results = await crawler.search_all_tweets_paginated(
        spec.query,
        start_time=iso_start,
        end_time=iso_end,
        max_results=max_results,
        max_pages=max_pages,
        page_pause=page_pause,
        label=window_label,
        language="ar",
    )

    if not results.tweets:
        logger.warning("âš ï¸ æ— ç»“æœ: %s (%s â†’ %s)", spec.label, iso_start, iso_end)
        return

    if results.metadata:
        results.metadata.label = window_label
        results.metadata.total_collected = results.result_count
        results.metadata.language = results.metadata.language or "ar"

    output_path = save_results(results, window_label, format=output_format)
    logger.success(
        "ğŸ’¾ ä¿å­˜å®Œæˆ: %s | æ¨æ–‡ %s æ¡ â†’ %s",
        window_label,
        results.result_count,
        output_path,
    )


async def async_main(args: argparse.Namespace) -> None:
    start = parse_date(args.start)
    # include the entire end date until 23:59:59 by adding one day and treating window as [start, end)
    end = parse_date(args.end) + timedelta(days=1)

    selected_specs = [
        spec
        for spec in DEFAULT_QUERIES
        if not args.queries or spec.label in args.queries
    ]

    if not selected_specs:
        raise SystemExit("æœªåŒ¹é…åˆ°ä»»ä½•æŸ¥è¯¢æ ‡ç­¾ï¼Œè¯·æ£€æŸ¥ --queries å‚æ•°")

    crawler = TwitterCrawler()

    try:
        for window_start, window_end in iter_windows(start, end, args.window_days):
            for spec in selected_specs:
                await backfill_window(
                    crawler,
                    spec,
                    window_start,
                    window_end,
                    max_pages=args.max_pages,
                    max_results=args.max_results,
                    page_pause=args.page_pause,
                    output_format=args.format,
                )
    finally:
        await crawler.close()


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="æŠ“å– 2025 ä¸­å›½ 93 é˜…å…µç›¸å…³çš„é˜¿è¯­æ¨æ–‡",
    )
    parser.add_argument("--start", default="2024-12-01", help="èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end", default="2025-12-31", help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--window-days", type=int, default=14, help="åˆ†å‰²æ—¶é—´çª—å£çš„å¤©æ•°")
    parser.add_argument("--max-pages", type=int, default=None, help="æ¯ä¸ªçª—å£æœ€å¤§åˆ†é¡µæ¬¡æ•°")
    parser.add_argument("--max-results", type=int, default=400, help="æ¯é¡µæŠ“å–æ•°é‡ (â‰¤500)")
    parser.add_argument("--page-pause", type=float, default=3.5, help="åˆ†é¡µé—´éš”ç§’æ•°")
    parser.add_argument(
        "--format",
        choices=("json", "jsonl"),
        default="json",
        help="ä¿å­˜æ ¼å¼",
    )
    parser.add_argument(
        "--queries",
        nargs="*",
        help="ä»…è¿è¡ŒæŒ‡å®šæ ‡ç­¾çš„æŸ¥è¯¢ (é»˜è®¤è¿è¡Œå…¨éƒ¨)",
    )
    return parser


def main() -> None:
    parser = build_parser()
    args = parser.parse_args()

    logger.info("ğŸš€ å¯åŠ¨é˜¿è¯­å†å²å›å¡«ä»»åŠ¡")
    asyncio.run(async_main(args))


if __name__ == "__main__":
    main()
