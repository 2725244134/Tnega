"""
============================================
æŠ“å– 93 é˜…å…µç›¸å…³æ¨æ–‡ï¼ˆçœŸå®æ•°æ®ï¼‰
============================================
ä½¿ç”¨è‹±è¯­æŸ¥è¯¢è¯æœç´¢ä¸­å›½ 2025 å¹´çºªå¿µæŠ—æˆ˜èƒœåˆ© 70 å‘¨å¹´é˜…å…µç›¸å…³å†…å®¹
æ³¨æ„ï¼šç”±äº search_all_tweets éœ€è¦ Academic Research æƒé™ï¼Œ
     æˆ‘ä»¬æ”¹ç”¨å…¶ä»–å¯ç”¨çš„ API æ–¹æ³•
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.x_crawl import TwitterCrawler, save_results, save_tweets_jsonl


async def crawl_recent_parade_topics():
    """æŠ“å– 93 é˜…å…µç›¸å…³æ¨æ–‡ï¼ˆä½¿ç”¨ search_recent_tweetsï¼Œæœç´¢æœ€è¿‘çš„ç›¸å…³å†…å®¹ï¼‰"""
    print("\n" + "=" * 60)
    print("ğŸ¯ ä»»åŠ¡ï¼šæœç´¢ä¸­å›½é˜…å…µç›¸å…³æ¨æ–‡ï¼ˆæœ€è¿‘ 7 å¤©ï¼‰")
    print("=" * 60)
    print("âš ï¸  æ³¨æ„ï¼šä½¿ç”¨ search_recent_tweets åªèƒ½æœç´¢æœ€è¿‘ 7 å¤©çš„æ¨æ–‡")
    print("ğŸ’¡ æç¤ºï¼šå¦‚æœéœ€è¦ 2015 å¹´çš„å†å²æ•°æ®ï¼Œéœ€è¦ Academic Research æƒé™")
    
    # è‹±è¯­æŸ¥è¯¢è¯ç»„åˆï¼ˆæœç´¢å½“å‰ç›¸å…³è¯é¢˜ï¼‰
    queries = [
        "China military parade",
        "China Victory Day",
        "Beijing parade",
    ]
    
    crawler = TwitterCrawler()
    
    try:
        for query in queries:
            print(f"\nâ–¶ï¸ æŸ¥è¯¢: {query}")
            print("-" * 60)
            
            try:
                # ä½¿ç”¨ search_recent_tweetsï¼ˆæœ€è¿‘ 7 å¤©ï¼Œä¸éœ€è¦ç‰¹æ®Šæƒé™ï¼‰
                results = await crawler.search_recent_tweets(
                    query=query,
                    max_results=50  # å…ˆè·å– 50 æ¡çœ‹çœ‹æ•ˆæœ
                )
                
                if results.result_count == 0:
                    print(f"âš ï¸ æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ¨æ–‡ï¼ˆå¯èƒ½æœ€è¿‘ 7 å¤©æ²¡æœ‰ç›¸å…³å†…å®¹ï¼‰")
                    continue
                
                print(f"âœ… æ‰¾åˆ° {results.result_count} æ¡æ¨æ–‡")
                
                # è¾“å‡ºå‰ 5 æ¡æ¨æ–‡æ¦‚è§ˆ
                print("\nğŸ“‹ æ¨æ–‡é¢„è§ˆï¼ˆå‰ 5 æ¡ï¼‰ï¼š")
                for i, tweet in enumerate(results.tweets[:5], 1):
                    author = results.users.get(tweet.author_id)
                    author_name = f"@{author.username}" if author else "æœªçŸ¥ç”¨æˆ·"
                    
                    # æˆªæ–­é•¿æ–‡æœ¬
                    text_preview = tweet.text[:80] + "..." if len(tweet.text) > 80 else tweet.text
                    
                    print(f"\n{i}. {author_name}")
                    print(f"   {text_preview}")
                    print(f"   â¤ï¸ {tweet.like_count:,} | ğŸ”„ {tweet.retweet_count:,} | ğŸ’¬ {tweet.reply_count:,}")
                    print(f"   ğŸ•’ {tweet.created_at}")
                
                # ç»Ÿè®¡æ•°æ®
                total_likes = sum(t.like_count or 0 for t in results.tweets)
                total_retweets = sum(t.retweet_count or 0 for t in results.tweets)
                total_replies = sum(t.reply_count or 0 for t in results.tweets)
                
                print(f"\nğŸ“Š ç»Ÿè®¡æ•°æ®ï¼š")
                print(f"   æ¨æ–‡æ•°: {results.result_count}")
                print(f"   ç”¨æˆ·æ•°: {len(results.users)}")
                print(f"   æ€»ç‚¹èµ: {total_likes:,}")
                print(f"   æ€»è½¬å‘: {total_retweets:,}")
                print(f"   æ€»å›å¤: {total_replies:,}")
                
                # ä¿å­˜æ•°æ®
                json_path = save_results(results, f"China_parade_{query.replace(' ', '_')}")
                jsonl_path = save_tweets_jsonl(results.tweets, f"parade_{query.replace(' ', '_')}.jsonl")
                
                print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜ï¼š")
                print(f"   JSON:  {json_path}")
                print(f"   JSONL: {jsonl_path}")
                
            except Exception as e:
                error_msg = str(e)
                if "429" in error_msg:
                    print("âŒ é€Ÿç‡é™åˆ¶ï¼ˆ429ï¼‰ï¼šAPI è°ƒç”¨è¿‡äºé¢‘ç¹")
                    print("ğŸ’¡ å»ºè®®ï¼šç­‰å¾… 15 åˆ†é’Ÿåé‡è¯•")
                    break
                elif "403" in error_msg:
                    print("âŒ æƒé™ä¸è¶³ï¼ˆ403ï¼‰")
                    print("ğŸ’¡ å»ºè®®ï¼šæ£€æŸ¥ API å¯†é’¥æƒé™")
                    break
                else:
                    print(f"âŒ é”™è¯¯: {error_msg}")
    
    finally:
        await crawler.close()


async def demonstrate_with_known_tweets():
    """
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å·²çŸ¥æ¨æ–‡ ID è·å–çœŸå®æ•°æ®
    ï¼ˆè¿™ç§æ–¹æ³•å¯ä»¥è·å–å†å²æ¨æ–‡ï¼Œä¸å— 7 å¤©é™åˆ¶ï¼‰
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ æ–¹æ¡ˆ 2ï¼šä½¿ç”¨å·²çŸ¥æ¨æ–‡ ID è·å–æ•°æ®")
    print("=" * 60)
    
    crawler = TwitterCrawler()
    
    try:
        # è·å– Twitter å†å²è‘—åæ¨æ–‡æ¼”ç¤º API åŠŸèƒ½
        print("\nâ–¶ï¸ è·å– Twitter å†å²æ¨æ–‡ï¼ˆæ¼”ç¤º API åŠŸèƒ½ï¼‰...")
        
        # Jack Dorsey ç¬¬ä¸€æ¡æ¨æ–‡
        demo_ids = ["20"]
        
        results = await crawler.get_tweets(demo_ids)
        
        if results.result_count > 0:
            print(f"âœ… API å·¥ä½œæ­£å¸¸ï¼æˆåŠŸè·å– {results.result_count} æ¡æ¨æ–‡")
            
            for tweet in results.tweets:
                author = results.users.get(tweet.author_id)
                print(f"\nğŸ“„ æ¨æ–‡å†…å®¹: {tweet.text}")
                print(f"   åˆ›å»ºæ—¶é—´: {tweet.created_at}")
                print(f"   ç‚¹èµæ•°: {tweet.like_count:,}")
                print(f"   è½¬å‘æ•°: {tweet.retweet_count:,}")
                if author:
                    print(f"   ä½œè€…: @{author.username} ({author.name})")
            
            # ä¿å­˜ç¤ºä¾‹æ•°æ®
            save_path = save_tweets_jsonl(results.tweets, "api_demo.jsonl")
            print(f"\nğŸ’¾ æ¼”ç¤ºæ•°æ®å·²ä¿å­˜: {save_path}")
            
            print("\nâœ… API åŠŸèƒ½éªŒè¯é€šè¿‡ï¼")
        
    except Exception as e:
        if "429" in str(e):
            print("âŒ é€Ÿç‡é™åˆ¶ï¼šè¯·ç­‰å¾… 15 åˆ†é’Ÿåé‡è¯•")
        else:
            print(f"âŒ é”™è¯¯: {e}")
    
    finally:
        await crawler.close()


async def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹çœŸå®æ•°æ®æŠ“å–")
    print("=" * 60)
    
    # æ–¹æ¡ˆ 1ï¼šæœç´¢æœ€è¿‘ 7 å¤©çš„ç›¸å…³è¯é¢˜
    await crawl_recent_parade_topics()
    
    # æ–¹æ¡ˆ 2ï¼šæ¼”ç¤ºä½¿ç”¨å·²çŸ¥ ID è·å–å†å²æ•°æ®
    print("\n" + "=" * 60)
    await demonstrate_with_known_tweets()
    
    # è¾“å‡ºæ€»ç»“
    print("\n" + "=" * 60)
    print("ğŸ“ æ€»ç»“")
    print("=" * 60)
    print("\nè¦è·å– 93 é˜…å…µï¼ˆ2015å¹´ï¼‰çš„çœŸå®æ¨æ–‡æ•°æ®ï¼Œä½ éœ€è¦ï¼š")
    print("\né€‰é¡¹ A - ä½¿ç”¨å·²çŸ¥æ¨æ–‡ IDï¼š")
    print("  1. åœ¨ Twitter ç½‘é¡µæœç´¢ 'China military parade 2015'")
    print("  2. æ‰¾åˆ°ç›¸å…³æ¨æ–‡ï¼Œä» URL å¤åˆ¶æ¨æ–‡ ID")
    print("     ä¾‹å¦‚ï¼štwitter.com/user/status/123456 â†’ ID æ˜¯ 123456")
    print("  3. æ·»åŠ åˆ°è„šæœ¬ä¸­ä½¿ç”¨ get_tweets(ids) è·å–")
    print("\né€‰é¡¹ B - ç”³è¯· Academic Research æƒé™ï¼š")
    print("  1. è®¿é—® https://developer.twitter.com/en/portal/petition/academic/is-it-right-for-you")
    print("  2. ç”³è¯· Academic Research æƒé™ï¼ˆéœ€è¦å­¦æœ¯/ç ”ç©¶ç”¨é€”ï¼‰")
    print("  3. è·å¾—æƒé™åä½¿ç”¨ search_all_tweets() æœç´¢å†å²æ•°æ®")
    print("\né€‰é¡¹ C - æœç´¢æœ€è¿‘ 7 å¤©çš„ç±»ä¼¼è¯é¢˜ï¼š")
    print("  1. ä½¿ç”¨ search_recent_tweets()ï¼ˆå·²åœ¨ä¸Šé¢æ¼”ç¤ºï¼‰")
    print("  2. å¯ä»¥æ‰¾åˆ°æœ€è¿‘çš„ç›¸å…³è®¨è®ºå’Œè¯é¢˜")
    print("\né€‰é¡¹ C - ä½¿ç”¨ç°æœ‰æ•°æ®é›†ï¼š")
    print("  1. å¯»æ‰¾å·²æœ‰çš„ 93 é˜…å…µæ¨æ–‡æ•°æ®é›†")
    print("  2. å¯¼å…¥åˆ°æˆ‘ä»¬çš„ç³»ç»Ÿä¸­è¿›è¡Œåˆ†æ")


if __name__ == "__main__":
    asyncio.run(main())

